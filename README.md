# Opinion-Spam-Detection-Using-Ensemble-Leaning

# 	Introduction
In the realm of business success and failure, customer satisfaction emerges as a pivotal factor. The reputation and contentment of customers can essentially make or break a business. Hence, heeding customer feedback diligently and promptly addressing their concerns becomes imperative. Yet, amidst the realm of online reviews, a new challenge arises - the potential for misuse by malicious actors. Therefore, distinguishing between authentic and fraudulent reviews becomes paramount. Detecting fake online reviews inherently becomes akin to a binary classification problem, lending itself to conventional machine learning or statistical approaches. However, the inherent complexities of natural language and latent features pose challenges to accuracy in this endeavor.

Utilizing the most precise classification algorithms is imperative to develop robust detection platforms for identifying deception. Traditional machine learning methods have been extensively employed in literature focused on uncovering deceptive or fake reviews, yielding relatively accurate outcomes for classification tasks. Notably, support vector machines (SVM) are often hailed as the top choice across various application domains. However, recent research is unveiling the superiority of "ensemble learning" methodologies, surpassing traditional SVM-based approaches. Particularly, newly developed ensemble learning techniques like gradient-boosting machines are exhibiting highly promising results in classification tasks.

Motivated by recent advancements in ensemble learning for classification tasks, this project delves into investigating, evaluating, and contrasting the efficacy of ensemble-based techniques against traditional machine learning algorithms. Specifically, ensemble learning methods such as bagging and boosting are integrated into various machine learning and deep learning classifiers, including logistic regression, decision trees, random forest, k-nearest neighbors, multinomial naive Bayes, support vector machines, and multilayer perceptrons. These integrated models are then compared against their conventional counterparts. The Amazon Verified Purchase dataset from Kaggle is utilized as the primary dataset for this study (available at: https://www.kaggle.com/datasets/akudnaver/amazon-reviews-dataset).
The structure of the project report is as follows:
•	Section 2: Literature Review
•	Section 3: Experimental Setup
•	Section 4: Results and Findings

Ensemble learning offers a powerful method to enhance the performance of underperforming classifiers by leveraging the collective output of multiple models, thereby achieving a more resilient classification. There are three primary approaches to constructing an ensemble learner:
•	Boosting: Typically involves sequentially training homogeneous base models, iteratively focusing on instances misclassified by previous models to improve overall performance.
•	Bagging: Involves training homogeneous base models in parallel on different subsets of the training data, often using techniques like bootstrap sampling, and then combining their outputs, typically through averaging or voting.
•	Stacking: Utilizes predominantly heterogeneous base models, trained in parallel, whose outputs are combined using a meta-model, which learns to weigh the predictions of individual base models optimally.
Ensemble methods offer improved predictions and mitigate overfitting by averaging or voting the outputs generated by a pool of classifiers, particularly beneficial for small datasets. Additionally, ensemble learning excels in avoiding local minima by leveraging multiple models, thereby expanding the search space and enhancing the likelihood of finding optimal outputs. Moreover, ensemble learning can address various machine learning challenges and limitations, including class imbalance, concept drift (changes in features or labels over time), and the curse of dimensionality.

The objectives of this project are as follows:
1) Investigate the performance of ensemble learning-based methodologies in comparison with conventional machine/deep learning approaches for detecting fake reviews.
2) Evaluate the effectiveness of ensemble-based learning compared to conventional machine learning algorithms.
3) Assess the improvement in accuracy resulting from hyperparameter optimization.

# 	Experimental Setup
Data Collection and Data Cleaning
The initial dataset downloaded from Kaggle had 32 columns, where it had more than enough information. This dataset is basically a collection of different feedback across Amazon Branded products. The idea is to gain some insight into Customer Reviews across these products and look for any improvement from negative reviews. Upon further analysis, the information within the dataset was focused more on the (1) product and (2) the reviews and not so much on the reviewer themselves, and hence it was subsequently concluded that this project will take the linguistics approach to the fake review detection problem, and not behavioral. The 30 other columns within this dataset, although a bit excessive for the purpose of this project, aided us in understanding the background of the reviews and the products in details, and thus was able to provide us with rich context, which we can refer to when we are analyzing our results down the project. Therefore, before dropping the 30 columns, graphs and analysis were made. After the analysis the only columns kept were review_text and verified_purchase, where they were saved inside a csv file, so we can conduct the EDA and Data Pre-processing on the textual data present.

Data Pre-processing and Document Embedding
In this project, all classifiers employed features extracted through Count Vectorization and TF-IDF (Term Frequency-Inverse Document Frequency). Count Vectorization converts text documents into numerical vectors representing the frequency of each term in the document, while TF-IDF further enhances this representation by weighing the importance of terms based on their frequency across the entire corpus. These techniques allow for the transformation of textual data into a format suitable for machine learning algorithms. Prior to vectorization, the dataset underwent preprocessing steps including the removal of special characters, stop words, and punctuation marks to ensure optimal feature extraction.

Classification Matrix
In this project we employed well-established classification evaluation metrics, encompassing accuracy, precision, recall, and F1 score. An accuracy score of 1.0 signifies complete alignment between predicted and actual labels. Precision reflects the ratio of true positives to all predicted positive labels, while recall indicates the ratio of predicted true positives to all observed true positive labels. The F1 score, a balanced measure of precision and recall, provides a holistic assessment of model performance, particularly useful when precision and recall exhibit discrepancies. While we report both accuracy and F1 score, our primary emphasis lies on accuracy for evaluating overall model efficacy.

Hyperparameter Optimization
In this project, we develop a python script to implement the chosen classifiers, leveraging the Scikit-learn library alongside XGBoost for Extreme Gradient Boosting Trees. Numerous experiments were conducted to pinpoint the most effective hyperparameters for each classifier. Let X denote the samples and y their corresponding labels, with C representing a classifier with l hyperparameters awaiting optimization, and H constituting a collection where each element i contains either a probability distribution for the values of the i-th hyperparameter of C, or an array of equiprobable categorical values for the hyperparameter. A randomized search involves the selection of hyperparameter values from H for a fixed number of iterations, constructing a C instance with these values, and performing classification. In this endeavor, we executed the randomized search 100 times, preserving the model obtained in each iteration for future evaluation. Furthermore, each model underwent fitting using K-fold cross-validation, with K set to 10 for each cycle. Upon the conclusion of all iterations, the optimal model was chosen based on accuracy, and both the model itself and its optimal hyperparameters were returned. The number of models estimated during the randomized search is bounded by the iteration count; however, fewer models are estimated if the potential combinations of hyperparameters are limited. This method also safeguards against the creation of duplicate sets of hyperparameters.
The probability distribution over the values, or the possible
categorical values, of the classifiers’ hyperparameters, follows:
1) Logistic Regression
• C: discrete uniform distribution over [0.1, 1000).
• Solver: newton-cg, lbfgs, liblinear, sag, saga.
2) Decision Tree
• Max depth: discrete uniform distribution over [3, 5). This range was obtained empirically considering the size of our dataset, in order to prevent over-fitting.
• Splitter: best, random.
• Criterion: gini, entropy.
3) Random Forest
• Number of weak classifiers: discrete uniform distribution
over [100, 1000).
• Max depth: discrete uniform distribution over [3, 5).
• Criterion: gini, entropy.
4) K Nearest Neighbors
• n_neighbors: range[1, 21)
• Weights: uniform, distance.
• Algorithm: auto, ball_tree, kd_tree, brute.
5) Multinomial Naive Bayes
• Alpha: Sequence [0.1, 2) increment 0.1.
• Fit_prior: True, False.
6) Support Vector Machine
• Kernel: RBF, linear, polynomial, sigmoid.
• C: continuous uniform over [0.1, 1000).
• Degree: discrete uniform distribution over [3, 10).
• Gamma (γ): scale, auto. Only considered when the kernel is RBF.
7) Extreme Gradient Boosting Trees
• Number of stages: discrete uniform distribution over
[100, 1000).
• Max depth: discrete uniform distribution over [3, 5).
• Gamma (γ):continuous uniform distribution over
[0, 10).
• Learning rate (α): continuous uniform distribution
over [0.01, 1).
8) Multilayer Perceptron
• Maximum iterations: uniformly sampled from the set {600, 800, 1000, 1200}.
• Hidden layers size: sampled from a discrete uniform distribution over [2, 5). The number of units in each layer is uniformly sampled, once per layer, from {5, 10, 15, 20, 25, 30, 35, 40, 45, 50}.
• Activation function: ReLU, hyperbolic tangent, logistic sigmoid, identity function.

Ensemble using Bagging and AdaBooost
Ensemble of each classifier were trained fitted with their respective best hyperparameters. We used bagging and AdaBoost for the ensemble training. In terms of AdaBoost, we omitted the KNN and MLP as these classifiers do not support sample_weight which is essential for training AdaBoost. We also omitted XGBoost classifier which already is an ensemble boosting technique.
